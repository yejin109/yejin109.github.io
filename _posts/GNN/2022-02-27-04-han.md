---
title: "[Paper] Heterogeneous Graph Attention Network"
toc: true
use_math: true
categories:
  - GNN
tags:
  - [Attention, GNN]
---

"Neural Network for Graphs: A Contextual Constructive Approach"이란 논문에 대한 리뷰입니다.

원문은 [링크](https://dl.acm.org/doi/abs/10.1145/3308558.3313562?casa_token=31HAUQjNolEAAAAA:kMu4yG_a7cPQGxjIvAabOlOqLWg-337XeT7MxtJfE-Jyze8nQmHNB9S7qZl0oLuVmnmhB27lhoE)에서 확인할 수 있습니다.


# 특징
Node level과 semantic level attention 모두 사용한다.
- Node level은 node와 meta-path based neighbors로 정해져 node에 weight를 주게 된다. 
- Semantic level은 specific task에 맞춰서 meta-path에게 weight를 주게 된다.

  
# 전반적인 schema

Type 별로 embedding 생성
![제목](/assets/images/GNN/han_1.PNG){: width="70%" height="60%"}{: .align-center} 
 
Node level attention을 진행하고
![제목](/assets/images/GNN/han_2.PNG){: width="20%" height="20%"}{: .align-center} 
 
softmax로 normalization
![제목](/assets/images/GNN/han_3.PNG){: width="20%" height="20%"}{: .align-center} 

node level attention과 neighbor의 embedding으로 node의 embedding 생성
![제목](/assets/images/GNN/han_4.PNG){: width="30%" height="30%"}{: .align-center} 
  
이걸 nonlinear activation을 거치고 다시 softmax하여 normalization
![제목](/assets/images/GNN/han_5.PNG){: width="20%" height="20%"}{: .align-center} 

즉 앞서 node level embedding과 semantic level embedding을 다시 합치기
![제목](/assets/images/GNN/han_6.PNG){: width="30%" height="30%"}{: .align-center} 

loss는 cross entropy로 진행하고 semi 환경에 대한 l index이다.
![제목](/assets/images/GNN/han_7.PNG){: width="20%" height="20%"}{: .align-center} 



