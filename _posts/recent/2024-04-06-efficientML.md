---
title: "[Survey] Recent approaches about Efficient ML "
toc: false
toc_sticky: false
toc_lable: "Main Contents"
use_math: true
categories:
  - Survey
tags:
  - [LLM, EfficientML]
---

This is a collection of recent approaches and papers about Efficient ML including Parameter Efficient Fine Tuning(PEFT), qunatization, pruning and other topics.

# PEFT
- MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning([arxiv](https://arxiv.org/abs/2403.20320))
- LayerNorm: A key component in parameter-efficient fine-tuning([arxiv](https://arxiv.org/abs/2403.20284))
- ReFT: Representation Finetuning for Language Models([arxiv](https://arxiv.org/abs/2404.03592))
- LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning([arxiv](https://arxiv.org/abs/2403.17919))
- GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection([arxiv](https://arxiv.org/abs/2403.03507?utm_source=substack&utm_medium=email))
- LORAPRUNE: PRUNING MEETS LOW-RANK PARAMETER-EFFICIENT FINE-TUNING([openreview](https://openreview.net/pdf?id=9KVT1e1qf7))
- LoRA+: Efficient Low Rank Adaptation of Large Models([arxiv](https://arxiv.org/abs/2402.12354?utm_source=substack&utm_medium=email))

# Quantization
- Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models([arxiv](https://arxiv.org/abs/2404.02837))
- QLoRA: Efficient Finetuning of Quantized LLMs([arixv](https://arxiv.org/abs/2305.14314))

# Pruning
- Random Search as a Baseline for Sparse Neural Network Architecture Search([arxiv](https://arxiv.org/abs/2403.08265))
- The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models([arxiv](https://arxiv.org/pdf/2403.03942.pdf))