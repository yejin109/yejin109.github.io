---
title: "[Survey] Recent approaches about Super alignments "
toc: false
toc_sticky: false
toc_lable: "Main Contents"
use_math: true
categories:
  - Survey
tags:
  - [LLM, Alignment]
---

This is a collection of recent approaches and papers about super-alignment and relevant topics.

# Key paper
- Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision([OpenAI Blog](https://openai.com/research/weak-to-strong-generalization), [arxiv](https://arxiv.org/abs/2312.09390)) 
- Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning([arxiv](https://arxiv.org/abs/2402.00667))
- Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts([arxiv](https://arxiv.org/abs/2402.15505))

# RL based approach 

- PPO, Proximal policy optimization algorithms([arxiv](https://arxiv.org/abs/1707.06347))
- Deep reinforcement learning from human preferences([arxiv](https://arxiv.org/abs/1706.03741))
- Learning to summarize from human feedback([arxiv](https://arxiv.org/abs/2009.01325))

- Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences([arxiv](https://arxiv.org/abs/2403.07230))

- Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards([arxiv](https://arxiv.org/abs/2403.07708))

# Principle

- Understanding the Learning Dynamics of Alignment with Human Feedback([arxivg](https://arxiv.org/abs/2403.18742))
- On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models([arxiv](https://arxiv.org/abs/2403.04204))

# Learning algorithm

- Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models([arxiv](https://arxiv.org/abs/2401.01335))

- Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision([arxiv](https://arxiv.org/abs/2403.09472))

- The Unreasonable Effectiveness of Easy Training Data for Hard Tasks([arxiv](https://arxiv.org/abs/2401.06751))

# Other Approaches

- Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models([arxiv](https://arxiv.org/abs/2402.03749))
- Tuna: Instruction Tuning using Feedback from Large Language Models([arxiv](https://arxiv.org/abs/2310.13385))
- Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment([arxiv](https://arxiv.org/abs/2402.10207))
- Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective([arxiv](https://arxiv.org/abs/2402.10184))
- Weak-to-Strong Jailbreaking on Large Language Models([arxiv](https://arxiv.org/abs/2401.17256))