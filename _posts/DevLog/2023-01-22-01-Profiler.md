---
title: "[DevLog] Profiler"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
  - Dev Log
tags:
- [Profiling]
---

# Overview

When it comes to  preparel service launching with ML based models, optimization over all components of system are required. So to speak, reducing an unnecessary energy usage is important and energy that ML based model usually uses is time and memory of server. Especially Deep learning based model exploits a lot of energy to run a tremendous floating operations. To do so, GPUs are mainly used  that is the key point of this post.

IT-based industries have used various management approaches and regarding the server side management, profiling tools are devised for it own sake. However, our interest of service uses GPUs which requires a huge computation power for floating operations repeatedly. So we need to care about the memory size and time consumption it uses for a single operation. Additionally, how many time single computation call a internal functions including value copy or allocating memeory for GPU.

# Approaches
## Python Decorator

So there are serveral trials to cursoring the energy usage. From the native point of view, we can manually implement logging memeory usage and runtime.  Codes for measure time to running user defined operation might like :


```
check time code

@check_time
def forward():

```                                       

we can use this kind of decorator for torch based model or even user defined function! Surely it is useful but we need more! We need to memory that designated function allocates and deallocates.  Regarding GPU usage, most of us are familiar with 

```
!nvidia-smi
```

But we'd like to use this line anytime we want, even in the process of running whole lines. Python also offers for this kind of process using **subprocess**. Here is the example. 

```

```

To implement using this line, we need to know about each command line that we like to use like 'nvidia-smi'. 

Now, we can measure time to run a specific function and memory usage once that function have run. But we cannot detect internal function call that pytorch uses. To do so we need to update every function including even package defined function! 

## torch.Profiler

Good news! Pytorch offers torch oriented profiler! Even we can measure how many time evey internal function had been called for every operation. User can define the code block to measure and separately measure time and memory usage. Here is the example

```
code
```

```
results
````

As you can see the above samples, it offers a lot of information. To be optimal, we can use this profiler when it comes to understanding model architecture. How the brand-new model optimized outdated codes and improved memory usages. Even we can detect unnecessary copy which  are frequently called because of code lines. 


                                                                                                             

## Reference

[Python-subprocess](https://docs.python.org/3/library/subprocess.html)
[Nvidia-smi pdf](https://www.google.com/url?sa=t&source=web&rct=j&url=https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf&ved=2ahUKEwjrlby3zdr8AhUEY94KHZNvDTAQFnoECCUQAQ&usg=AOvVaw09aO-EC4f9tvnQ63Y78vsC)  [torch Profiler Tutorial](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
