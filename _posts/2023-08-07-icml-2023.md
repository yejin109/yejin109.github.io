---
title: "[Conference] ICML 2023 Highlights"
toc: false
toc_sticky: false
toc_lable: "Main Contents"
use_math: true
categories:
  - Conference
tags:
  - [ICML]
---

ICML 2023 Highlights!

지난 7월 23일부터 7월 29일까지 ICML 2023이 진행되었습니다! 그 중에서 관심을 가지고 볼만한 paper들을 요약한 포스트들을 공유드리려고 합니다.


1. Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models
  - [Notion Link](https://yejin109.notion.site/Arithmetic-Sampling-Parallel-Diverse-Decoding-for-Large-Language-Models-a74320e288d44c9b8bd78233273fd7c3?pvs=4)
2. RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank
  - [Notion Link](https://yejin109.notion.site/RankMe-Assessing-the-Downstream-Performance-of-Pretrained-Self-Supervised-Representations-by-Their--1fc7bcc7f01646c89c82251cef43054c?pvs=4)
3. Evaluating Self-Supervised Learning via Risk Decomposition
  - [Notion Link](https://yejin109.notion.site/Evaluating-Self-Supervised-Learning-via-Risk-Decomposition-8effadc1e908451b8b07d0b32cc84d05?pvs=4)
4. Learning-Rate-Free Learning by D-Adaptation
  - [Notion Link](https://yejin109.notion.site/Learning-Rate-Free-Learning-by-D-Adaptation-1ceeb9c3a4eb44a2b3efb1aa14cfacf1?pvs=4)
5. Which Features are Learned by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression
  - [Notion Link](https://yejin109.notion.site/Which-Features-are-Learned-by-Contrastive-Learning-On-the-Role-of-Simplicity-Bias-in-Class-Collapse-6ae6570e00b946efbbcdeb05450d07d1?pvs=4)
6. Tutorial : ML Theory
  - [Notion Link](https://yejin109.notion.site/Recent-Advances-in-the-Generalization-Theory-of-Neural-Networks-96de5f302b24439792ceb26886b837a1?pvs=4)
