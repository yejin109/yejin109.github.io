---
title: "[Paper] Big Self-Supervised Models are Strong Semi-Supervised Learners"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
  - Self Supervised
tags:
  - [self-supervised]
---

"Big Self-Supervised Models are Strong Semi-Supervised Learners"이란 논문에 대한 리뷰입니다.

원문은 [링크](https://arxiv.org/abs/2006.10029)에서 확인할 수 있습니다.

# Key
-	Distillation 단계가 더 들어간다.
 - 단, smaller network에서 학습을 진행해야 한다. 이는 overfit과도 연관되어 보인다.

-	simCLR v1에선 encoder output을 representation으로 사용하지만, 여기선 middle layer의 representation(projection head)을 사용하도록 한다. 
 - 특히 label이 적을 때 그러하다.

-	simCLR보다 deeper but less wide ResNet을 사용한다.

# Architecture
-	Pre-training: task-agnostic unsupervised  general representation
-	Fine tuning: supervised
-	Distillation: task specific unsupervised with pseudo labels from fine-tuned network (teacher network)
 -  Unlabelled가 다시 사용되면서 compact model을 사용하여 성능을 올린다. 
 - Loss를 정의할 때 entropy loss를 사용하고 이 때 teacher network에서 뱉은 label의 확률을 닮도록 학습한다. 즉, teacher label이 나오는 term은 constant이며 student network만 학습시키는 것.
 - 그리고 일부의 labelled는 student network을 학습하는데 사용하고 labelled와 unlabelled는 ratio로 조정해준다. 

#Insight
-	The fewer the labels are, the more it benefits from a bigger model.
 - Label이 적어서 결국 overfit을 안하는 것일 수도 있다.
-	Bigger/Deeper projection heads improve representation learning
 - Middle layer가 좋다는 얘기
 - 특히 ResNet이 클수록 성능 gain은 줄어든다.
 - 즉, 이미 충분히 wide하다면, depth의 영향은 제한적이다.
