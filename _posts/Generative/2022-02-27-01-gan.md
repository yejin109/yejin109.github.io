---
title: "[Paper] Generative Model Basic"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
  - Generative
tags:
  - []
---

"Generative Adversarial Nets"이란 논문에 대한 리뷰입니다.

원문은 [링크](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)에서 확인할 수 있습니다.

# Key:
기존의 연구들은 
 - inference를 위해 결국 normalization이 필요하게 되고, 이건 intractable
 -이를 대체하기 위한 MCMC도 결국 Time complexity가 크다.

GAN은
 - Inference가 없다. 즉 explicit representation of generator does not exist
 - 단, 적절히 학습되어야 한다.(Discriminator와 Generator가 적절한 수준으로 학습되어야)

# Architecture
-	Noise variables Z (latent variable) : 바로 sample을 generate할 수 없어 단순한 형태(gaussian같은)의 분포를 따르는 값들이다.
-	Generator(mapping from Z to X space) : Sample distribution의 Cardinality로 mapping
-	Discriminator: Data Sample과 Generated Sample을 구분하는 함수. Domain은 Sample space

#Insight
-	D의 gradient는 input으로 X에 대해선 ascending, Z에 대해선 descending 형태.
  - 이건 결국 X space 상에서 Sample이 있어야 하는 곳을 구분지어주는 것

-	G의 gradient는 D가 Z를 input으로 받을 때 확률을 높혀
 - 이건 결국 noise를 X space 상으로 올릴 때 Sample이 있는 곳들(유사하게)로 보냄
 - 그래서 Generator 학습시 한 곳으로 보내는Helvetica scenario 조심해야한다

-	학습 순서에 차이가 있다.
 - D가 먼저 poor한 G로부터 잘 구분해서 X space 상에서 decision boundary를 만든다.
 - 그 이후 G는 그 decision boundary에 들어가도록 noise를 mapping 하는 함수를 배운다.
 - 그래서 먼저 k번 Discriminator를 학습 하고 1번 Generator를 학습시키는 것이다.

-	Discriminator는 결국 data sample에 대한 frequentist적 확률로 정의되고 이게 optimal이다.

-	Generator의 optimal은 x의 분포랑 같아지는 것이다.
 - 그래서 궁극적으론 Generator를 얻을 수 있게 되고 이게 목표가 되는것
 - Discriminator는 단순히 1/2가 되는 것.
 - 특히 G에 대해서만 본다면, Loss가 convex하여 수렴하게 되는 것
