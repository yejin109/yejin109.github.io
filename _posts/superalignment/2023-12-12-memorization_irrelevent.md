---
title: "[Survey] Recent approaches about Super alignments "
toc: false
toc_sticky: false
toc_lable: "Main Contents"
use_math: true
categories:
  - Super Alignment
tags:
  - [LLM, Alignment]
---

This is a collection of recent approaches and papers about super-alignment.

Key paper
- Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision([OpenAI Blog](https://openai.com/research/weak-to-strong-generalization), [arxiv](https://arxiv.org/abs/2312.09390)) 
- The Unreasonable Effectiveness of Easy Training Data for Hard Tasks([arxiv](https://arxiv.org/abs/2401.06751))


Approaches
- Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models([arxiv](https://arxiv.org/abs/2402.03749))
- Tuna: Instruction Tuning using Feedback from Large Language Models([arxiv](https://arxiv.org/abs/2310.13385))
- Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment([arxiv](https://arxiv.org/abs/2402.10207))
- Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective([arxiv](https://arxiv.org/abs/2402.10184))
- Weak-to-Strong Jailbreaking on Large Language Models([arxiv](https://arxiv.org/abs/2401.17256))