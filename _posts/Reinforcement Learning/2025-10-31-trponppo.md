---
title: "[Paper Review] TRPO and PPO"
toc: true
use_math: true
categories:
  - Reinforcement Learning
  - Policy Optimization
tags:
  - [RL]
---


# ğŸ§  Paper Review: TRPO & PPO

ìì„¸í•œ ë¦¬ë·° ë‚´ìš©ì€ [slide link](https://drive.google.com/file/d/1SjKqt9ekHamBxHfHMshMoLlOSlysL29J/view?usp=sharing)ì—ì„œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ğŸ” Recap: Policy Gradient
- Reinforcement Learningì—ì„œ Policy GradientëŠ” **Monte Carlo Approximation**ì„ ì´ìš©í•´ ê¸°ëŒ€ ë³´ìƒì„ ì¶”ì •.
- **Reward-to-go** í˜•íƒœë¡œ í‘œí˜„í•˜ì—¬ ê° í–‰ë™ì˜ ë¯¸ë˜ ë³´ìƒë§Œ ê³ ë ¤.
- í•˜ì§€ë§Œ **ì¶”ì • ê³¼ì •ì— ë…¸ì´ì¦ˆê°€ ì¡´ì¬**í•˜ì—¬ í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ.

### ğŸ¯ Variance Reduction
- **Baseline term**ì„ ë„ì…í•˜ë©´ ë¶„ì‚°ì„ ì¤„ì´ë©´ì„œë„ unbiased estimatorë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŒ.
- Baselineì€ íŒŒë¼ë¯¸í„° Î¸ì™€ ë…ë¦½ì .

---

## âš™ï¸ Motivation
Policy Gradientì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì ‘ê·¼ì´ ì œì•ˆë¨:

1. **Parameter Space Regularization**  
   - íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ëŸ‰ì„ ì§ì ‘ ê·œì œ (linearization ê¸°ë°˜).
2. **Policy Space Regularization**  
   - ì •ì±… ê°„ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ê·œì œ (ì¦‰, í–‰ë™ ë¶„í¬ì˜ ë³€í™” ì œí•œ).

> âš ï¸ ë‹¨, íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì •ê·œí™”ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ parameterizationì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ â†’ ì •ì±… ê³µê°„ì—ì„œì˜ ì •ê·œí™”ê°€ ë” ì¼ë°˜ì .

---

## ğŸš€ TRPO (Trust Region Policy Optimization)

### ğŸ“˜ Theoretical Foundations
TRPOëŠ” Kakade & Langford (2002)ì˜ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨:
> â€œApproximately optimal approximate reinforcement learning.â€

- ìƒˆë¡œìš´ ì •ì±…ì˜ ê¸°ëŒ€ ë³´ìƒì€ **ê¸°ì¡´ ì •ì±…ì˜ advantage í•¨ìˆ˜**ë¥¼ í†µí•´ í‘œí˜„ ê°€ëŠ¥.
- ë‹¨, ë‘ ì •ì±…ì´ ì¶©ë¶„íˆ â€œê°€ê¹Œìš´â€ ê²½ìš°ì—ë§Œ ê·¼ì‚¬ê°€ ìœ íš¨.
- TRPOëŠ” ì •ì±… ê°„ì˜ ì°¨ì´ë¥¼ **KL Divergence**ë¡œ ì œí•œí•˜ëŠ” constrained optimizationìœ¼ë¡œ ì ‘ê·¼í•¨.

### âš–ï¸ Optimization Formulation
ìµœì¢… ëª©ì  í•¨ìˆ˜:
\[
\max_\theta \; \hat{E}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} A_t \right]
\]
subject to:
\[
\hat{E}_t [ KL(\pi_{\theta_{old}}(\cdot|s_t) \| \pi_\theta(\cdot|s_t)) ] \le \delta
\]

- **KL ì œì•½ ì¡°ê±´**ì€ â€œtrust regionâ€ì„ í˜•ì„±í•˜ì—¬ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë³´ì¥.
- TRPOëŠ” **on-policy í•™ìŠµ**ì´ì§€ë§Œ, old policyì˜ ë°ì´í„°ë¡œ ê·¼ì‚¬í•˜ë¯€ë¡œ **semi-off-policy** ì„±ê²©ë„ ê°€ì§.

---

## ğŸ§© PPO (Proximal Policy Optimization)

### ğŸ¯ Motivation
- TRPOì˜ constrained optimizationì€ ê³„ì‚°ì´ ë³µì¡í•˜ê³ , Î²(ë¼ê·¸ë‘ì£¼ ê³„ìˆ˜)ì˜ ì„¤ì •ì´ ë¬¸ì œì„.
- PPOëŠ” **unconstrained optimization**ìœ¼ë¡œ ë‹¨ìˆœí™”í•˜ë©´ì„œ TRPOì˜ ì•ˆì •ì„±ì„ ìœ ì§€í•˜ë ¤ í•¨.

### ğŸ” Approach
- **Probability ratio** \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \)
- **Clipped Objective:**
  \[
  L^{CLIP}(\theta) = \hat{E}_t [\min(r_t(\theta) A_t, \; clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t)]
  \]

- ì´ë•Œ `clip`ì€ í™•ë¥  ë¹„ìœ¨ì´ \(1-\epsilon\)ê³¼ \(1+\epsilon\) ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ.
- ê²°ê³¼ì ìœ¼ë¡œ, **exploit-prone update**ë¥¼ ë°©ì§€í•˜ê³  ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ ë„.

### ğŸ§ª Experiments
- ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ PPOëŠ” TRPOë³´ë‹¤ ë‹¨ìˆœí•˜ë©´ì„œë„ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±.
- clipping factor \( \epsilon \)ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ë¬¸ì œì— ë”°ë¼ ì¡°ì • í•„ìš”.

---

## ğŸ§  Summary

| í•­ëª© | TRPO | PPO |
|------|------|------|
| ëª©ì  | ì‹ ë¢°ì˜ì—­ ë‚´ ì •ì±… ì—…ë°ì´íŠ¸ | í´ë¦¬í•‘ëœ ëª©ì í•¨ìˆ˜ë¡œ ê·¼ì‚¬ |
| ì œì•½ | KL-divergence ì œì•½ | Unconstrained (Clipping) |
| ê³„ì‚° ë³µì¡ë„ | ë†’ìŒ | ë‚®ìŒ |
| ì•ˆì •ì„± | ë†’ìŒ | ë†’ìŒ |
| ì‹¤ìš©ì„± | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |

---

## ğŸ“š Reference
- Schulman et al., â€œTrust Region Policy Optimizationâ€, ICML 2015  
- Schulman et al., â€œProximal Policy Optimization Algorithmsâ€, arXiv 2017  
- Kakade & Langford, â€œApproximately Optimal Approximate Reinforcement Learningâ€, ICML 2002

---

