---
title: "[Paper Review] TRPO and PPO"
toc: true
use_math: true
categories:
  - Reinforcement Learning
  - Policy Optimization
tags:
  - [RL]
---


# 🧠 Paper Review: TRPO & PPO

자세한 리뷰 내용은 [slide link](https://drive.google.com/file/d/1SjKqt9ekHamBxHfHMshMoLlOSlysL29J/view?usp=sharing)에서 확인가능합니다.

---

## 🔁 Recap: Policy Gradient
- Reinforcement Learning에서 Policy Gradient는 **Monte Carlo Approximation**을 이용해 기대 보상을 추정.
- **Reward-to-go** 형태로 표현하여 각 행동의 미래 보상만 고려.
- 하지만 **추정 과정에 노이즈가 존재**하여 학습이 불안정할 수 있음.

### 🎯 Variance Reduction
- **Baseline term**을 도입하면 분산을 줄이면서도 unbiased estimator를 유지할 수 있음.
- Baseline은 파라미터 θ와 독립적.

---

## ⚙️ Motivation
Policy Gradient의 학습 안정성을 높이기 위해 두 가지 접근이 제안됨:

1. **Parameter Space Regularization**  
   - 파라미터의 변화량을 직접 규제 (linearization 기반).
2. **Policy Space Regularization**  
   - 정책 간의 차이를 직접 규제 (즉, 행동 분포의 변화 제한).

> ⚠️ 단, 파라미터 기반 정규화는 네트워크의 parameterization에 따라 달라질 수 있음 → 정책 공간에서의 정규화가 더 일반적.

---

## 🚀 TRPO (Trust Region Policy Optimization)

### 📘 Theoretical Foundations
TRPO는 Kakade & Langford (2002)의 결과를 기반으로 함:
> “Approximately optimal approximate reinforcement learning.”

- 새로운 정책의 기대 보상은 **기존 정책의 advantage 함수**를 통해 표현 가능.
- 단, 두 정책이 충분히 “가까운” 경우에만 근사가 유효.
- TRPO는 정책 간의 차이를 **KL Divergence**로 제한하는 constrained optimization으로 접근함.

### ⚖️ Optimization Formulation
최종 목적 함수:
\[
\max_\theta \; \hat{E}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} A_t \right]
\]
subject to:
\[
\hat{E}_t [ KL(\pi_{\theta_{old}}(\cdot|s_t) \| \pi_\theta(\cdot|s_t)) ] \le \delta
\]

- **KL 제약 조건**은 “trust region”을 형성하여 학습의 안정성을 보장.
- TRPO는 **on-policy 학습**이지만, old policy의 데이터로 근사하므로 **semi-off-policy** 성격도 가짐.

---

## 🧩 PPO (Proximal Policy Optimization)

### 🎯 Motivation
- TRPO의 constrained optimization은 계산이 복잡하고, β(라그랑주 계수)의 설정이 문제임.
- PPO는 **unconstrained optimization**으로 단순화하면서 TRPO의 안정성을 유지하려 함.

### 🔍 Approach
- **Probability ratio** \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \)
- **Clipped Objective:**
  \[
  L^{CLIP}(\theta) = \hat{E}_t [\min(r_t(\theta) A_t, \; clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t)]
  \]

- 이때 `clip`은 확률 비율이 \(1-\epsilon\)과 \(1+\epsilon\) 범위를 벗어나지 않도록 제한.
- 결과적으로, **exploit-prone update**를 방지하고 안정적인 학습을 유도.

### 🧪 Experiments
- 다양한 환경에서 PPO는 TRPO보다 단순하면서도 비슷하거나 더 나은 성능을 달성.
- clipping factor \( \epsilon \)은 하이퍼파라미터로, 문제에 따라 조정 필요.

---

## 🧠 Summary

| 항목 | TRPO | PPO |
|------|------|------|
| 목적 | 신뢰영역 내 정책 업데이트 | 클리핑된 목적함수로 근사 |
| 제약 | KL-divergence 제약 | Unconstrained (Clipping) |
| 계산 복잡도 | 높음 | 낮음 |
| 안정성 | 높음 | 높음 |
| 실용성 | 중간 | 매우 높음 |

---

## 📚 Reference
- Schulman et al., “Trust Region Policy Optimization”, ICML 2015  
- Schulman et al., “Proximal Policy Optimization Algorithms”, arXiv 2017  
- Kakade & Langford, “Approximately Optimal Approximate Reinforcement Learning”, ICML 2002

---

