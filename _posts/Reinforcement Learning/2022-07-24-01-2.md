---
title: "[KMOOC 강화학습] Week 01-2 Multi-armed Bandit 문제"
toc: true
use_math: true
categories:
  - Reinforcement Learning
tags:
  - [KMOOC]
---

해당 강의는 K-MOOC의 "강화학습의 수학적 기초와 알고리즘 이해" 수업을 수강하며 기록한 내용입니다. 강의는 [링크](http://www.kmooc.kr/courses/course-v1:KoreaUnivK+ku_ai_002+2020_A44/course/)에서 확인하실 수 있습니다.


# Multi-armded bandit 문제란

- 환경 : 여러 슬롯 머신이 있고 각 bandit machine에 대한 상태(State)정보는 부재
- 행동 : 하나를 선택하여 당김.
- 보상 : 동일한 돈이 나오지 않고 슬롯마다 금액이 다른 보상.

## 해결책(일반적인 강화학습 방법)

1. Exploration : 아무것도 모르는 상태에서 랜덤하게 탐색 시작
2. Exploitation : 누적된 시도의 보상을 보고 의사 결정

## 행동과 목적

주어진 횟수에서 총 보상을 최대화하는 슬롯 머신을 선택


## 의사 예시

- 학습 주체(agent) : 의사
- 환경 : 어떤 약이 좋은 지 모름
- 행동 : 임상시험 중에 약 처방
- 보상 : 복용 후 증상으로 좋고 나쁜 점 확인

🌟 결론

- 학습 주체(agent)가 k개의 행동(action) 중 하나를 선택
- 선택한 행동에 보상(rewar)을 받는 일련의 과정 수행
- **일정 기간동안 취득한 보상의 총합**에 대한 기대값을 최대화하도록 어떤 행동을 취할지 결정


# 행동 가치(action value)

어떠한 행동을 취했을 때 보상에 대한 기댓값

$q(a) = E[R_{t} \mid A_{t} = a] = \sum_{r} p (r \mid a) r$


⚡ 다만 학습주체가 행동가치에 대한 분포를 모른다! 

**이것을 모르기에** k-armed bandit의 challenge가 되며 행동 가치 함수를 **try & error로 추정**할 수 있게 된다면 좋은 action을 취할 수 있게 된다.

## 추정 방법

표본 평균 방법(Sample-mean method) : 전체 보상 합 / 선택한 횟수
  - $Q_{n+1} = {1\over n} \sum_{i=1}^{n} R_{i} = Q_{n} + {1\over n}(R_{n} - Q_{n}) $
    - 이 방법으로 계속 보상값을 tracking하여 **정보를 업데이트** 하면 된다.
    - 그냥 직전의 정보만 가지고 있으면 된다!

단 지금의 상황은 모든 단계에서의 보상의 가중치가 동일하다는 가정에서 평균치를 계산하게 된다. 하지만 **시간에 따라** 보상의 가치(weight)가 달라질 수 있다. 이런 경우를 **Non-stationary**한 상황이라고 한다.

## 비정상성 문제

$Q_{n+1} = Q_{n} + \alpha_{n}(R_{n}-Q_{n})$ 

## 추정치 업데이트 방식

$V \gets V + \alpha (\hat{V}-V)$

기존의 추정치에 새로운 정보와 기존의 추정치의 차이만큼 반영하여 gradually 반영한다. 이 때의 반영 정도는 weight($\alpha$)가 된다.

$V \gets (1-\alpha) V + \alpha \hat{V}$

기존 정보와 새로운 정보의 가중평균