---
title: "[KMOOC 강화학습] Week 01-3 강화학습 맛보기"
toc: true
use_math: true
categories:
  - Reinforcement Learning
tags:
  - [KMOOC]
---

해당 강의는 K-MOOC의 "강화학습의 수학적 기초와 알고리즘 이해" 수업을 수강하며 기록한 내용입니다. 강의는 [링크](http://www.kmooc.kr/courses/course-v1:KoreaUnivK+ku_ai_002+2020_A44/course/)에서 확인하실 수 있습니다.

# Q 러닝

$Q(s_{t},a_{t}) \gets (1-\alpha)Q(s_{t},a_{t}) + \alpha(r_{t+1} \gamma \max_{a} Q(s_{t+1},a)), \alpha=1, \gamma=1$

- $\alpha$: 과거에 대해 얼마나 고려할 것인지에 대한 비율
- $\gamma$ : 감가율

🌟 기존까지의 추정치에 새로 얻어진 정보로 업데이트하는 것으로 Bandit과 방식은 유사하지만 다만 **대상**은 어떻게 될까?

## 디티일 

어떤 경로에 따라 최대의 누적 보상을 얻는지 학습하며 방향성을 제시하는 것이 바로 $Q$에 해당하며 Q는 **누적 보상의 최대값에 대한 추정치**로 정의된다.

- 환경 : 그리드에 대한 정보는 없다.
- State : 로봇의 현재의 위치(1,2,3,4,5,6)
- Action : 움직이는 방향(화살표)

그 결과 Q는 State에서 Action에 따른 최대 누적 보상의 기대치를 말하게 된다. 

⚡ 한 Action에 따른 누적 보상합의 최대값을 활용하여 그 action을 취했을 때 얻을 수 있는 누적 보상의 최대값을 추정한다.
  - 즉, 어떤 action에 따른 reward를 추정하고 싶은데 **그 행동 다음의 Q의 상한선**의 일정 부분을 이용한다는 것?!

☝ 이 때 한 operation(iteration)이 끝나면 그것을 episode라고 하며 어떻게 한 episode로 업데이트 한다!

🌟 여러 Episode로 학습하여 얻은 Q를 바탕으로 가장 높은 Q를 얻게 되는 Action을 선택하는 가이드를 얻게 된다!

## Question

🙋‍♂️ Episode 의 구성은 어떻게 되는가?