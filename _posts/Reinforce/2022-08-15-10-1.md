---
title: "[KMOOC 강화학습] Week 10-1 Temporal Difference Learning"
toc: true
use_math: true
categories:
  - Reinforcement Learning
tags:
  - [KMOOC]
---

해당 강의는 K-MOOC의 "강화학습의 수학적 기초와 알고리즘 이해" 수업을 수강하며 기록한 내용입니다. 강의는 [링크](http://www.kmooc.kr/courses/course-v1:KoreaUnivK+ku_ai_002+2020_A44/course/)에서 확인하실 수 있습니다.

# 개괄

환경에 대한 정보(상태 전이 확률) + 액션에 대한 리워드가 밝혀져있다면, MDP문제로 치환되고 풀 수 있다.

다만 환경에 대한 정보, 보상에 대한 정보가 없는 경우 상호작용으로 풀고자 한다.

그래서 크게 2가지로 구성된다.

- Prediction: 정책에 대한 상태 가치 함수를 계산, 평가하는 과정
- Control: 개선된 정책을 생성하고 최적 정책을 찾아가는 과정

## MC

에피소드가 종료될 때까지 진행하고, 모든 상태에 대한 리턴 값을 계산하여 가치함수를 업데이트 하게 된다.

- Prediction: 누적 리워드 합을 계산하는 과정에 해당하며 이 때 방법으로 First Visit과 Every visit, incremental 방식이 있었다.
- Control: 행동가치함수에 대한 추정에 해당하며, 하나의 에피소드를 바탕으로 행동-가치함수를 업데이트 하게 되고 정책을 다시 업데이트 해나가는 방식이다. 이 때 정책을 업데이트 하는 방식은 epsilon greedy 방식을 사용한다.

![사진](/assets/images/RL/w10-01-01.PNG){: width="80%" height="70%"}{: .align-center}