---
title: "[Paper] PD-GAN"
toc: true
use_math: true
---

"PD-GAN: Adversarial Learning for Personalizaed Diversity-Promoting Recommendation"이란 논문에 대한 리뷰입니다.

원문은 [링크](https://www.ijcai.org/proceedings/2019/0537.pdf)에서 확인할 수 있습니다.

# 1. Introduction
> Trade-off between relevance and diversity of recommendation lists
아이템 리스트의 diversity를 높이다 보면 relevance가 낮아지는 문제가 생기게 되고, 이를 해결하려는 것이 personalized diversity promoting recommendation model .

본 모델에서 다루는 personal preference는 2가지로 다음과 같습니다.
- personal preference to individual items
- personal preference to the diversity of a set of items
특히 ground truth에 속한 각각의 아이템은 individual time에 대한 선호도를, ground truthd 집합 자체는 diversity에 대한 선호도를 반영하게 됩니다.

본 모델은 GAN framework를 사용하여 Personalizaed Diversity-promoting GAN이라고 불리며 특히 GAN의 generator는 DPP model을 바탕으로 구성되며 특징은 다음과 같습니다.
- Personalized -> 각 유저마다 DPP kernel matrix를 구성
- user's preferenence to item -> 아이템과의 relevance를 학습
- co-occurence of items -> 아이템의 diversity를 학습

<!-- ## Co-occurence of item
1. Ground truth 데이터 셋 구축
모든 유저가 본 데이터들 중에서 random sampling하여 diverse set만듭니다. 

다만 많이 본 아이템이 많이 뽑힐 가능성이 있기 때문에 diverse set마다 비슷한 아이템을 random sample하여 similar set도 만들게 됩니다.

이렇게 한  -->

# 3. PD-GAN Model
## 데이터 셋 환경
M명의 user와 N개의 item 그리고 C개의 item category로 구성을 하고, 각 아이템은 최소한 하나의 item category를 가지게 됩니다.

각 유저 $u$ 마다 random sampling한 observed diverse set $T = \lbrace i_{1}, ...\rbrace$ 를 만드는데 이 때 각각의 아이템은 서로 다른 category들을 가지게 됩니다. 

다양한 아이템을 본 사용자(eclectic)의 T의 cardinality는 커지게 되고 일부 아이템만 본 사용자(focused)의 T의 cardinality는 작을 수 밖에 없습니다.
> The cardinality of T will naturally be larger for eclectic users than that for focused users.

이와 같은 방식으로 각 사용자마다 여러개의 diverse set을 만들 수 있고 다음과 같이 표기하며 ground truth로 사용하게 됩니다. 
> $\mathcal{T}_{u} = \lbrace T_{1}, T_{2},... \rbrace $

이렇게 구성하였을 때 <br>
각각의 **아이템들**은 observed data에서 뽑았으니 해당 사용자의 특정 아이템에 대한 선호도 정보를 가지고 있으며, <br>
각각의 **아이템 셋**은 data의 category 구성에 따라 뽑았으니 해당 사용자의 diversity에 대한 선호도 정보를 가지고 있다고 합니다.

## 모델 구조 
앞서 구축한 ground truth와 유사하게 diverse하고 relevant한 추천을 진행하는 것이 바로 모델의 개념적인 원리입니다.

이렇게 작동하기 위해서 adversarial learning framework를 사용하게 됩니다.

1. 각 사용자마다 generator는 **모든 아이템**을 대상으로 Maxtrix Factorization(MF)를 통해 relevance를 평가합니다.
2. diversity를 알려주는 diverse item의 co-occurence를 반영하는 pre-learnt DPP를 준비합니다.
3. relevance 평가 결과와 pre-learnt DPP를 합친 modefied DPP 모델에서 순차적으로 top-K item을 sampling하고 이를 Generator의 결과물입니다.
4. DIscriminator는 Generaot가 만든 top-k item과 grount truth를 구분하도록 학습합니다.

> The generator will be able to generate a set of diverse and relevant items so similar to the ground-truth that discriminator will not be able to distinguish easily

ground truth와 유사한 아이템 셋을 뱉어 내는 것이니 앞서 diversity는 아이템 셋으로 학습한다는 것이 것이 말이 된다고 생각합니다. 

## Diversity(pre-learnt DPP) 학습 방법
discreset set $\mathcal{L}$에 subset T에 대해서 pre-learnt DPP는 다음과 같이 정의됩니다.
$\mathcal{P}(T) \propto  \det(L_{T})$
$\mathcal{P}(T) = {\det(L_{T}) \over \det(L+I)}$

L-ensemble을 사용한다고 할 때 N이 커질 수록 계산 복잡도가 매우 커지므로 Low rank factorization을 진행하게 됩니다.
$L =AA^{T}$ where A is $N \times D $

L-ensemble을 학습하는 과정은 다음과 같습니다.
1. 전체 사용자들의 ground truth에서 몇개의 diverse set을 sampling하게 됩니다 .
    - $\mathcal{T} = \lbrace T_{1}, T_{2}, ... T_{X}\rbrace$ 
2. Gradient decent방식으로 $\log \mathcal{P}(\mathcal{T}\mid A)$을 maximizing하여 L을 학습합니다.

이 방식을 통해 diverse item 중에서 더 많이 같히 뽑힐수록 더 높은 확률을 가지도록 학습하게 됩니다. 

다만 이 방식의 문제점은 많은 사람들이 본 아이템은 많이 뽑힌다는 문제가 존재해서 다음과 같은 방식으로 학습을 하게 됩니다.

1. 앞서 뽑은 diverse set마다 동일한 카테고리를 가지는 아이템으로 구성된 similary set $\tilde{T}$를 만듭니다.
    - $< \mathcal{T},\tilde{T} > = \lbrace < T_{1},\tilde{T}_{1} >, < T_{X},\tilde{T}_{X} >$
2. 다음 objective를 최적화 한다면 두번째 항을 통해서 유사한 아이템이 등장할 확률을 줄일 수 있습니다.
    - $ \begin{aligned} \max \mathcal{J} &= \log \mathcal{P}(\matchcal{T}\mid A) - \log \mathcal{P}(\tilde{T}\mid A) \\
                                        &= \sum_{x=1}^{X} \log \mathcal{P}(\matchcal{T}_{x}\mid A) - \log \mathcal{P}(\tilde{T}_{x}\mid A) \\
                                        &= \sum_{x=1}^{X} \log \det(L_{[T_{X}]}) - \log \det(L_{[\tilde{T}_{X}]})\end{aligned}$
    
## Sampling
DPP에 필요한 L-ensemble을 학습한 다음 순차적으로 top-K개의 아이템을 sampling을 진행하게 되고 본 논문에서는 fast greedy MAP inference를 통해 진행한다고 합니다. 

## Relevance 학습방법
pre-learnt DPP과 달리 Relevance를 합친 modified DPP는 다음과 같은 특징이 있습니다. 
$\mathcal{P}(T) \propto \alpha \sum_{i\in T}\mathcal{Q}(u,i) + (1-\alpha) \det(L_{T})$
    - $\mathcal{Q}(u,i)$는 quality-evaluating function으로 사용자마다 아이템과의 relevance를 나타내는 함수이며 $\alph$는 relevance와 diversity를 조정하는 hyperparameter로 사용됩니다.

이 때 MF를 이용하여 quality evaluating function을 정의하면 다음과 같습니다.
    - $\mathcal{Q}(u,i) = \sigma (v_{u}v_{i}^{T}), v_{u}&v_{i}$ 는 $1\times D$ embedding vector이며 $\sigma$는 sigmoid function을 말합니다.

그 결과 각 사용자ㅕ 마다 modified DPP의 kernel은 다음과 같이 정의됩니다.
    - 각 아이템들에 대해서 quality evaluating function을 구해서 하나의 벡터로 만들고 이를 $\mathcal{Q}(u)$라고 합니다.
    - $L_{u} = Diag(\exp (\beta $\mathcal{Q}(u)$)) * L *  Diag(\exp (\beta $\mathcal{Q}(u)$)) $
    - $\beta = {\alpha\over 2(1-\alpha)}$