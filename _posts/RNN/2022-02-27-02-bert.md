---
title: "[Paper] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
  - RNN
tags:
  - [Recurrent NN]
---

"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"이란 논문에 대한 리뷰입니다.

원문은 [링크](https://arxiv.org/abs/1810.04805)에서 확인할 수 있습니다.

# Key
-	Bidirectional Transformer

구조가 bidirectional하게 되는 것보단, 학습 방법때문에 bidirectional하게 되는 것이다.

-	Pre-train 

Trained on unlabeled data over different pre-training task

이 때 진행하는 task가 Masked LM과 Next Sentence Prediction이다. <br>
이 때 사용하는 dataset은 말 그대로 Language Model을 만들기 위한 것으로 사람들이 흔히 사용하는 표현들을 학습하는 것<br>
pretrain이 끝나게 되면 문장에 대한 representation 즉 fixed size vector를 얻을 수 있다. 이는 적용하고 싶은 task에 맞춰서 representation을 생성하면 될 것 같다.

-	Fine tuning
Initialized with the pre-trained and fine tuning using labeled data from downstream task<br>
본 단계는 특정 task를 위해 조정하는 방식이다.<br>
즉 pretrain 모델 뒤에 fc layer를 붙여서 분류를 할 수 있게 되는 것이다.<br>
즉 pretrain에서 하는task는 단지 bidirectional한 context를 반영하기 위한것이고 실제 적용할 땐 이렇게 원하는 task로 갈 수 있다.<br>
만약 분류를 한다면 CLS를 사용하게 된다.

- Pre-process
 -	Single sentence and pair of sentence
Fine tuning 할 때 사용하는 downstream task가 다양하니 필요에 따라 single sentence나 pair를 input으로 사용한다. 그 결과 input이 되는 sequence는 하나 혹은 2개의 sentence다.<br>
그래서 pretrain을 위한 전처리할 때에 허깅페이스에서 제공하는 것을 사용한다면, 두 문장 사이에는 ￦t를 넣고, 마지막엔 ￦n을 넣어서 전처리하면 된다. 혹은 전체 긴 문장을 크게 2개로 나눠서 전처리해도 된다.<br>
Fine tuning을 위한 전처리를 할 때엔 2개의 문장을 붙여서 사용하는 것이 아니라서 segmentation embedding은 동일한 embedding을 사용하면 된다. 그래서 fine tuning할 때 segment index list는 동일한 값들이고, token index list는 token에 대한 index를 넣어주면 될 것 같다. <br>

-	Special Tokens
 - CLS
모든 sequence의 시작은 [CLS]라는 token이 추가되어야 하고, 이건, classification을 할 때 사용된다. 
 - SEP
두 문장을 사용하는 경우 이 token으로 구분짓는다. 이를 통해 첫번째 sentence A와 두번째 sentence B를 구분하는 embedding을 가지게 된다. 
 - Etc.
문장이나 맥락에서 특정한 의미를 가지는 것(가격, 전화번호 등등)은 일관된 token으로 정리해야한다.
 -	Tokenizer
Wordpiece 모델을 사용하는 것은 다국어에 적용하기 위함이지만, 단일어를 하기 위해선 mecab과 같이 한국어에 맞춰진 tokenizer를 사용해도 좋다. <br>
혹은 sentencepeice를 사용하면 더 빠르기도 하다.
 -	MLM
Randomly masks some of the tokens in the input. 이 때 각 seq마다 전체 voca의 15%를 random으로 masking하게 된다.

# Architecture
-	Transformer와 동일한 구조
-	Input
  - Token Embedding
  -  Segment Embedding : whether A or B
  -  Position Embedding : order in the sequence

# Insight
-	Bidirectional 성질을 학습하는 방법
Pre-training 단계에 MLM task에선 masked를 output으로 사용해서 예측하게 된다.<br>
NSP task의 경우 50%가 sentence B가 실제 다음 문장이며(IsNext) 50%는 가짜 문장이다. <br>
Fine tuning 단계에선 task에 따라서 data set이 구성이 된다. <br>


결국 일반적인 텍스트 데이터에 대해 pre train하고 난 다음 fine tuning할 때 특정 task(downstream task)에 맞춰서 조정하게 된다.
